import streamlit as st
import numpy as np
import torch
import tensorflow as tf
import os
import io
import scipy.io.wavfile as wavfile
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏™‡∏°‡∏≠‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (Therapy Engine - PyTorch) ---
class TherapyAI:
    def __init__(self, policy_path=None, llm_path=None):
        self.is_live = False
        if policy_path and llm_path and os.path.exists(policy_path):
            try:
                self.policy_model = torch.load(policy_path)
                self.tokenizer = AutoTokenizer.from_pretrained(llm_path)
                self.llm = AutoModelForCausalLM.from_pretrained(llm_path)
                self.is_live = True
            except: pass

    def get_response(self, text):
        # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡πÉ‡∏ä‡πâ RL Policy ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Strategy
        # ‡πÅ‡∏ï‡πà‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏à‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Valence (V) ‡πÅ‡∏•‡∏∞ Arousal (A) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        v, a = 0.5, 0.5
        msg = "‡∏â‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏£‡∏±‡∏ö‡∏ü‡∏±‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏™‡∏°‡∏≠‡∏Ñ‡∏£‡∏±‡∏ö"
        
        if "‡πÄ‡∏®‡∏£‡πâ‡∏≤" in text: v, a, msg = 0.2, 0.3, "‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏£‡∏ô‡∏∞ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏®‡∏£‡πâ‡∏≤‡∏à‡∏∞‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ ‡∏ü‡∏±‡∏á‡πÄ‡∏û‡∏•‡∏á‡∏ô‡∏µ‡πâ‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"
        elif "‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î" in text: v, a, msg = 0.3, 0.8, "‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏•‡∏∂‡∏Å‡πÜ ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏•‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏∞‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ú‡πà‡∏≠‡∏ô‡∏Ñ‡∏•‡∏≤‡∏¢"
        elif "‡∏î‡∏µ" in text: v, a, msg = 0.8, 0.6, "‡∏î‡∏µ‡πÉ‡∏à‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏≤‡∏â‡∏•‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏ó‡∏≥‡∏ô‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏î‡πÉ‡∏™‡∏Å‡∏±‡∏ô"
        
        return msg, v, a

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏°‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á (Music Synthesis - TensorFlow) ---
class MusicAI:
    def __init__(self, rnn_path=None, vocoder_path=None):
        self.is_live = False
        if rnn_path and vocoder_path and os.path.exists(rnn_path):
            try:
                self.rnn_model = tf.keras.models.load_model(rnn_path)
                self.vocoder = tf.keras.models.load_model(vocoder_path)
                self.is_live = True
            except: pass

    @tf.function(experimental_relax_shapes=True)
    def synthesize_sound(self, v, a):
        sample_rate = 44100
        duration = 5.0
        t = np.linspace(0, duration, int(sample_rate * duration))
        
        # --- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö High-Fidelity ---
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ï‡∏≤‡∏° Valence (‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå)
        freq = 130.81 if v < 0.5 else 261.63 # C3 (‡∏ó‡∏∏‡πâ‡∏°) ‡∏´‡∏£‡∏∑‡∏≠ C4 (‡πÉ‡∏™)
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏£‡∏¥‡∏á (Harmonic Addition)
        # ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏ï‡πà‡∏ú‡∏™‡∏°‡∏´‡∏•‡∏≤‡∏¢ Layer ‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏´‡∏ô‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏à‡∏£‡∏¥‡∏á
        audio = 1.0 * np.sin(2 * np.pi * freq * t)
        audio += 0.4 * np.sin(2 * np.pi * (freq * 2) * t) # Octave
        audio += 0.2 * np.sin(2 * np.pi * (freq * 3.01) * t) # Overtones
        
        # 2. ‡πÉ‡∏™‡πà ADSR Envelope (‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏µ‡πÅ‡∏£‡∏á‡∏õ‡∏∞‡∏ó‡∏∞‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏î‡πÄ‡∏õ‡∏µ‡∏¢‡πÇ‡∏ô/‡∏Å‡∏µ‡∏ï‡∏≤‡∏£‡πå)
        envelope = np.exp(-1.2 * t) 
        audio = audio * envelope
        
        # 3. ‡πÉ‡∏™‡πà Reverb (‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏¥‡∏ï‡∏¥‡πÄ‡∏™‡∏µ‡∏¢‡∏á)
        reverb_delay = int(sample_rate * 0.05)
        audio[reverb_delay:] += 0.3 * audio[:-reverb_delay]
        
        return audio, sample_rate

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÅ‡∏≠‡∏õ (Streamlit UI) ---
st.set_page_config(page_title="Therapy Music AI", layout="centered")
st.title("üéπ AI Therapy Music Studio")

# ‡πÇ‡∏´‡∏•‡∏î Engine
if 'therapy' not in st.session_state:
    st.session_state.therapy = TherapyAI()
    st.session_state.music = MusicAI()

# ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏Ñ‡∏∏‡∏¢
user_msg = st.chat_input("‡∏£‡∏∞‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢...")

if user_msg:
    # 1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢ Therapy AI
    response_text, v, a = st.session_state.therapy.get_response(user_msg)
    
    with st.chat_message("assistant"):
        st.write(response_text)
        
        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ Music AI
        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏±‡∏á‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡∏ô‡∏ï‡∏£‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì..."):
            audio_wave, sr = st.session_state.music.synthesize_sound(v, a)
            
            # Mastering & Export
            buf = io.BytesIO()
            # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡πâ‡∏ô‡∏∏‡πà‡∏°‡∏ô‡∏ß‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å
            audio_out = (audio_wave / np.max(np.abs(audio_wave)) * 32767).astype(np.int16)
            wavfile.write(buf, sr, audio_out)
            
            st.audio(buf, format="audio/wav")
            st.caption(f"Mood Detected: {v} | Strategy: RLHF-Optimized")
